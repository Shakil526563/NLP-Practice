{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYVY1UjSDzl91TM6hyiYQW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tUdCubqNp-FA"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Perfect! Here's a **complete NLP preprocessing pipeline** that includes:\n","\n","‚úÖ Lowercasing\n","‚úÖ Punctuation + number removal\n","‚úÖ Tokenization\n","‚úÖ Stopword removal\n","‚úÖ POS tagging (keep only **nouns** and **verbs**)\n","‚úÖ Lemmatization\n","‚úÖ Named Entity Removal (optional)\n","‚úÖ TF-IDF Vectorization\n","‚úÖ Ready for machine learning models like Logistic Regression, SVM, etc.\n","\n","---\n","\n","## ‚úÖ Step-by-Step NLP Preprocessing Pipeline\n","\n","```python\n","import re\n","import nltk\n","from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag, ne_chunk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tree import Tree\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Downloads\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('omw-1.4')\n","\n","# Initialize tools\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Map POS tags for lemmatization\n","def get_wordnet_pos(tag):\n","    if tag.startswith(\"J\"):\n","        return wordnet.ADJ\n","    elif tag.startswith(\"V\"):\n","        return wordnet.VERB\n","    elif tag.startswith(\"N\"):\n","        return wordnet.NOUN\n","    elif tag.startswith(\"R\"):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","# Named Entity filtering\n","def remove_named_entities(tagged_tokens):\n","    chunked = ne_chunk(tagged_tokens)\n","    words_only = []\n","    for chunk in chunked:\n","        if isinstance(chunk, Tree):  # It's a named entity\n","            continue\n","        else:\n","            words_only.append(chunk[0])\n","    return words_only\n","\n","# Full preprocessing function\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # Remove URLs\n","    text = re.sub(r\"\\d+\", \"\", text)                    # Remove numbers\n","    text = re.sub(r\"[^\\w\\s]\", \" \", text)               # Remove punctuation\n","    text = re.sub(r\"\\s+\", \" \", text).strip()           # Remove extra spaces\n","\n","    tokens = word_tokenize(text)\n","    tokens = [t for t in tokens if t not in stop_words]\n","\n","    tagged = pos_tag(tokens)\n","\n","    # Keep only Nouns and Verbs\n","    tagged = [t for t in tagged if t[1].startswith(\"N\") or t[1].startswith(\"V\")]\n","\n","    # Remove named entities (optional)\n","    words = remove_named_entities(tagged)\n","\n","    # Lemmatize\n","    lemmatized = [\n","        lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged if word in words\n","    ]\n","    \n","    return \" \".join(lemmatized)\n","```\n","lemmatized = [\n","    lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged\n","]\n","\n","\n","1. tagged:\n","This is a list of tokens from the text that have been part-of-speech tagged.\n","\n","Each element in tagged is a tuple: (word, pos), where word is a token (like \"running\") and pos is the part-of-speech tag (like \"VB\" for verb, \"NN\" for noun, etc.).\n","\n","Example tagged output:\n","\n","python\n","Copy\n","Edit\n","[('The', 'DT'), ('cats', 'NNS'), ('are', 'VBP'), ('running', 'VBG'), ('faster', 'RBR')]\n","Here, 'cats' is a plural noun ('NNS'), 'are' is a verb ('VBP'), and 'running' is a gerund or verb form ('VBG').\n","---\n","\n","## ‚úÖ Example: Use with TF-IDF\n","\n","```python\n","docs = [\n","    \"Shakil is learning Natural Language Processing at DIU!\",\n","    \"The university offers great NLP courses.\",\n","    \"I love studying machine learning and Python.\",\n","    \"Sometimes the lectures are boring and long.\"\n","]\n","\n","# Apply preprocessing to each document\n","cleaned_docs = [preprocess(doc) for doc in docs]\n","\n","# Vectorize\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(cleaned_docs)\n","\n","# Check output\n","print(\"TF-IDF Feature Names:\")\n","print(vectorizer.get_feature_names_out())\n","\n","print(\"\\nTF-IDF Matrix:\")\n","print(X.toarray())\n","```\\\n","\n","---\n","\n","## ‚úÖ Output Example\n","\n","```\n","TF-IDF Feature Names:\n","['boring' 'cours' 'learn' 'lectur' 'love' 'machin' 'offer' 'process' 'python' 'shakil' 'studi' 'univers']\n","\n","TF-IDF Matrix:\n","[[...], [...], ...]\n","```\n","\n","---\n","\n","## ‚úÖ Next Step (Optional)\n","\n","Would you like to:\n","\n","* üéØ Train a **classifier** (e.g., Logistic Regression)?\n","* üß™ Split into train/test sets?\n","* üìä Evaluate using accuracy or confusion matrix?\n","\n","Let me know how you'd like to continue!\n"],"metadata":{"id":"K2qQXw9Sp-vd"}},{"cell_type":"markdown","source":["Sure! Let's walk through this `remove_named_entities` function line by line. It's used to **filter out named entities** (like person names, organizations, locations, etc.) from tokenized and POS-tagged text.\n","\n","---\n","\n","## üîß Function Purpose\n","\n","```python\n","def remove_named_entities(tagged_tokens):\n","```\n","\n","This function takes **POS-tagged tokens** (like `[('John', 'NNP'), ('is', 'VBZ')]`) and removes any **named entities** such as names, places, dates, etc.\n","\n","---\n","\n","## üß± Step-by-step Explanation\n","\n","### 1. **NER Chunking**\n","\n","```python\n","chunked = ne_chunk(tagged_tokens)\n","```\n","\n","* `ne_chunk` performs **Named Entity Recognition (NER)**.\n","* It takes **POS-tagged tokens** as input.\n","* It returns a **tree structure** (from `nltk.tree.Tree`) where named entities are grouped together as subtrees.\n","\n","**Example Input:**\n","\n","```python\n","[('John', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n","```\n","\n","**Output (chunked tree):**\n","\n","```\n","(S\n","  (PERSON John/NNP)\n","  lives/VBZ\n","  in/IN\n","  (GPE New/NNP York/NNP))\n","```\n","\n","* `PERSON` and `GPE` (Geo-Political Entity) are named entities.\n","\n","---\n","\n","### 2. **Initialize Output List**\n","\n","```python\n","words_only = []\n","```\n","\n","We will store all **non-named entity words** here.\n","\n","---\n","\n","### 3. **Loop through Chunks**\n","\n","```python\n","for chunk in chunked:\n","```\n","\n","Loop through each chunk in the `chunked` tree. A chunk can be:\n","\n","* A **subtree (named entity)**, e.g., `(PERSON John)`\n","* A **regular tuple**, e.g., `('lives', 'VBZ')`\n","\n","---\n","\n","### 4. **Check if It's a Named Entity**\n","\n","```python\n","if isinstance(chunk, Tree):  # It's a named entity\n","    continue\n","```\n","\n","* If `chunk` is a `Tree`, it represents a **named entity** (like \"John\" or \"New York\").\n","* So we **skip** it using `continue`.\n","\n","---\n","\n","### 5. **If Not a Named Entity, Keep the Word**\n","\n","```python\n","else:\n","    words_only.append(chunk[0])\n","```\n","\n","* If it's a regular token (not a named entity), we add the **word itself** (`chunk[0]`) to our list.\n","* `chunk[0]` is the **word**, `chunk[1]` is its POS tag.\n","\n","---\n","\n","### 6. **Return Clean List**\n","\n","```python\n","return words_only\n","```\n","\n","You now have a list of words with all named entities removed.\n","\n","---\n","\n","## ‚úÖ Example\n","\n","Input:\n","\n","```python\n","tagged = [('John', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n","remove_named_entities(tagged)\n","```\n","\n","Output:\n","\n","```python\n","['lives', 'in']\n","```\n","\n","Explanation:\n","\n","* `'John'` and `'New York'` are named entities, so they are removed.\n","\n","---\n","\n","## üìå Summary\n","\n","This function helps in cleaning text by **removing proper nouns** like:\n","\n","* Names (e.g., \"Elon Musk\")\n","* Places (e.g., \"Paris\")\n","* Organizations (e.g., \"Google\")\n","\n","This is useful if:\n","\n","* You don‚Äôt want personal or geographic bias in analysis.\n","* You're building general-purpose text models that shouldn't rely on names.\n","\n","Would you like to see this integrated into a full preprocessing pipeline?\n"],"metadata":{"id":"rw2QsqkRXuqx"}},{"cell_type":"markdown","source":["Great question!\n","\n","---\n","\n","## üß† Why use `WordNetLemmatizer()` in NLP?\n","\n","`WordNetLemmatizer` is used to **reduce a word to its base or dictionary form**, known as the **lemma**, in a way that **preserves the word‚Äôs actual meaning and part of speech**.\n","\n","---\n","\n","### ‚úÖ Example: Lemmatization vs Stemming\n","\n","| Word        | Stemming (`stemmer.stem()`) | Lemmatization (`lemmatizer.lemmatize()`) |\n","| ----------- | --------------------------- | ---------------------------------------- |\n","| **running** | runn                        | run                                      |\n","| **better**  | better                      | good                                     |\n","| **studies** | studi                       | study                                    |\n","| **ate**     | ate                         | eat                                      |\n","\n","üîç As you can see:\n","\n","* **Stemming** just chops off endings (it‚Äôs fast but crude).\n","* **Lemmatization** is **smarter**: it uses a dictionary (`WordNet`) to return meaningful words.\n","\n","---\n","\n","### üìö `WordNetLemmatizer` needs POS tag (optional but improves accuracy)\n","\n","```python\n","lemmatizer.lemmatize(\"running\")                     # 'running'\n","lemmatizer.lemmatize(\"running\", pos=\"v\")            # 'run' ‚úÖ\n","```\n","\n","---\n","\n","### ‚úÖ Why it‚Äôs better for machine learning:\n","\n","* **Improves model accuracy** by reducing word redundancy.\n","* Keeps valid words (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù).\n","* Useful in **search**, **text classification**, **sentiment analysis**, etc.\n","\n","---\n","\n","### üìå Summary:\n","\n","| Feature        | Stemming | Lemmatization            |\n","| -------------- | -------- | ------------------------ |\n","| Accuracy       | Low      | High                     |\n","| Language-aware | ‚ùå No     | ‚úÖ Yes (uses WordNet)     |\n","| Output Quality | Crude    | Clean, valid words       |\n","| Speed          | Fast     | Slower but more accurate |\n","\n","---\n","\n","Let me know if you want a visual comparison or to switch between stemming vs lemmatization in your project.\n"],"metadata":{"id":"59DEgwwZqPzd"}},{"cell_type":"code","source":[],"metadata":{"id":"fERglkOHqAIt"},"execution_count":null,"outputs":[]}]}
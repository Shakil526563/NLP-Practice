{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTNhwI1BWUjH6ZPgRI8S2W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crvwY1dPXRZf","executionInfo":{"status":"ok","timestamp":1747120429781,"user_tz":-360,"elapsed":52,"user":{"displayName":"MD Shakil","userId":"18384098869599648328"}},"outputId":"48c73597-73c3-4e4c-f608-a9295955973e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['hi', 'my', 'name', 'is', 'shakil', ',', 'now', 'i', 'am', 'studying', 'at', 'diu']\n","['hi', 'name', 'shakil', ',', 'studying', 'diu']\n","['hi', 'name', 'shakil', ',', 'studi', 'diu']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","sentence=\"hi my name is shakil,now i am studying at diu\"\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","# Download the 'punkt_tab' resource\n","nltk.download('punkt_tab') # This line is added to download the missing resource.\n","\n","tokenize=word_tokenize(sentence.lower())\n","\n","stopwords=set(stopwords.words('english'))\n","stopwordfilter=[word for word in tokenize if word not in stopwords]\n","\n","stemmer=PorterStemmer()\n","stemming=[stemmer.stem(word) for word in stopwordfilter]\n","\n","\n","\n","print(tokenize)\n","print(stopwordfilter)\n","print(stemming)"]},{"cell_type":"markdown","source":["Let's break down your **text preprocessing code** step by step so you fully understand what each line does. This is a very common and useful function in NLP to clean raw text before analysis.\n","\n","---\n","\n","### üî¢ Full Code:\n","\n","```python\n","stemmer = PorterStemmer()\n","stops = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","    tokens = [\n","        stemmer.stem(tok)\n","        for tok in text.split()\n","        if tok not in stops\n","    ]\n","    return \" \".join(tokens)\n","\n","docs_clean = [preprocess(d) for d in docs]\n","```\n","\n","---\n","\n","## ‚úÖ Line-by-Line Explanation\n","\n","---\n","\n","### üî∏ `stemmer = PorterStemmer()`\n","\n","* Creates a **PorterStemmer object**.\n","* The **Porter Stemmer** is a tool that **reduces words to their root form**.\n","\n","  * Example: `\"running\"` ‚Üí `\"run\"`, `\"flies\"` ‚Üí `\"fli\"`\n","\n","---\n","\n","### üî∏ `stops = set(stopwords.words('english'))`\n","\n","* Loads **English stopwords** from `nltk.corpus.stopwords`.\n","* Converts them into a `set` for **fast lookup**.\n","* Stopwords are common words like `\"is\"`, `\"the\"`, `\"and\"` ‚Äî they usually **don't add meaning** and are often removed.\n","\n","---\n","\n","### üî∏ `def preprocess(text):`\n","\n","* This defines a function called `preprocess` that takes in raw `text` and returns cleaned text.\n","\n","---\n","\n","### 1Ô∏è‚É£ `text = text.lower()`\n","\n","* Converts the entire text to **lowercase** to make analysis **case-insensitive**.\n","* Example: `\"Hello World\"` ‚Üí `\"hello world\"`\n","\n","---\n","\n","### 2Ô∏è‚É£ `text = re.sub(r\"[^\\w\\s]\", \"\", text)`\n","\n","* Removes **punctuation** using a **regular expression**.\n","* `r\"[^\\w\\s]\"` matches any character that is **not a word character** or **whitespace**.\n","* Example: `\"Hello, world!\"` ‚Üí `\"Hello world\"`\n","\n","---\n","\n","### 3Ô∏è‚É£ Tokenize + Remove Stopwords + Stem:\n","\n","```python\n","tokens = [\n","    stemmer.stem(tok)\n","    for tok in text.split()\n","    if tok not in stops\n","]\n","```\n","\n","* **`text.split()`** splits the sentence into **individual words (tokens)** by spaces.\n","* `if tok not in stops`: removes stopwords.\n","* `stemmer.stem(tok)`: applies stemming to each word.\n","\n","üìå Example:\n","\n","```python\n","text = \"Cats are running in the garden.\"\n","After stopword removal ‚Üí ['Cats', 'running', 'garden']\n","After stemming ‚Üí ['cat', 'run', 'garden']\n","```\n","\n","---\n","\n","### 4Ô∏è‚É£ `return \" \".join(tokens)`\n","\n","* Joins the cleaned tokens back into a single string.\n","\n","---\n","\n","### üîÅ `docs_clean = [preprocess(d) for d in docs]`\n","\n","* Applies the `preprocess()` function to **every document** in the list `docs`.\n","* Returns a list of cleaned text strings.\n","\n","---\n","\n","### üß† Summary of Steps in Preprocessing:\n","\n","| Step               | Purpose                     |\n","| ------------------ | --------------------------- |\n","| Lowercase          | Make text uniform           |\n","| Remove punctuation | Eliminate noise             |\n","| Tokenize           | Break into words            |\n","| Remove stopwords   | Eliminate unimportant words |\n","| Stem               | Reduce to word roots        |\n","| Rejoin tokens      | Return cleaned sentence     |\n","\n","---\n","\n","Would you like a diagram or want to see this function applied to a real example text?\n"],"metadata":{"id":"SmP5RE63iHx_"}},{"cell_type":"code","source":["import re #re (Regular Expressions) in NLP Preprocessing?\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# 1. Sample corpus + labels (for a simple sentiment task)\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","\n","# 2. Text Preprocessing\n","stemmer = PorterStemmer()\n","stops = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    # lowercase\n","    text = text.lower()\n","    # remove punctuation\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","    # tokenize + remove stopwords + stem\n","    tokens = [\n","        stemmer.stem(tok)\n","        for tok in text.split()\n","        if tok not in stops\n","    ]\n","    return \" \".join(tokens)\n","\n","docs_clean = [preprocess(d) for d in docs]"],"metadata":{"id":"i9JvciHxcgl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from nltk.stem import PortrerStemmer\n","from  nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","text=\"hi my name is shakil,now i am studying at diu\"\n","\n","stemm=PorterStemmer()\n","stop=set(stopwords.words('english'))\n","\n","def preprocess(text):\n","  text=text.lower()\n","  text=re.sub(r\"[^\\\\w\\s)\",\" \",text)\n","  tokens=[stemm.stem(word) for word in text.split() if word not in stop]\n","  return \" \".join(tokens)\n","\n","docs_clean=[preprocess(word) for word in text]\n"],"metadata":{"id":"bjLVbZURjg6-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Great! Here's an upgraded version of your `preprocess()` function that includes:\n","\n","‚úÖ **Lowercasing**\n","‚úÖ **Punctuation removal**\n","‚úÖ **Stopword removal**\n","‚úÖ **Tokenization**\n","‚úÖ **Number removal**\n","‚úÖ **Extra space cleanup**\n","‚úÖ **Lemmatization** (instead of stemming ‚Äî more accurate)\n","\n","---\n","\n","### üß† Requirements\n","\n","We‚Äôll use `WordNetLemmatizer`, so we must download a few NLTK resources:\n","\n","```python\n","import re\n","import nltk\n","from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","```\n","\n","---\n","\n","### ‚úÖ Full Preprocessing Function\n","\n","```python\n","def preprocess(text):\n","    # Lowercase\n","    text = text.lower()\n","    \n","    # Remove URLs (optional for web data)\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n","\n","    # Remove numbers\n","    text = re.sub(r\"\\d+\", \"\", text)\n","\n","    # Remove punctuation\n","    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n","\n","    # Remove extra spaces\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","\n","    # Tokenize\n","    tokens = word_tokenize(text)\n","\n","    # Stopword removal\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n","\n","    return \" \".join(lemmatized)\n","```\n","\n","---\n","\n","### ‚úÖ Test It\n","\n","```python\n","text = \"Hi! My name is Shakil. Now I'm studying at DIU since 2022.\"\n","print(preprocess(text))\n","```\n","\n","---\n","\n","### üîç Output Example\n","\n","```text\n","hi name shakil studying diu\n","```\n","\n","(Assuming \"diu\" is not removed ‚Äî you can manually add it to `stop_words` if needed.)\n","\n","---\n","\n","### üõ† Optional: Add Custom Stopwords\n","\n","```python\n","stop_words.update([\"diu\", \"name\"])\n","```\n","\n","---\n","\n","Let me know if you‚Äôd like to apply this to a **list of sentences**, build a **vectorizer**, or feed it into a model next!\n"],"metadata":{"id":"Nph8HhjBmMPG"}},{"cell_type":"markdown","source":["Excellent! Let's walk through **Optional Advanced NLP Preprocessing** in detail with working code examples:\n","\n","---\n","\n","## üîπ 1. **POS Tagging (Part-of-Speech Tagging)**\n","\n","We can use POS tagging to keep only **nouns**, **verbs**, etc., if we want to focus on content-heavy words.\n","\n","### ‚úÖ Example: Keep Only Nouns and Verbs\n","\n","```python\n","import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('averaged_perceptron_tagger')\n","\n","def filter_by_pos(text):\n","    tokens = word_tokenize(text)\n","    tagged = pos_tag(tokens)\n","    # Keep only nouns and verbs (NN*, VB*)\n","    filtered = [word for word, tag in tagged if tag.startswith(\"NN\") or tag.startswith(\"VB\")]\n","    return filtered\n","\n","text = \"Shakil is learning natural language processing at DIU.\"\n","print(filter_by_pos(text))  # ['Shakil', 'learning', 'language', 'processing', 'DIU']\n","```\n","\n","---\n","\n","## üîπ 2. **Named Entity Recognition (NER)**\n","\n","NER identifies **names, places, organizations, dates**, etc.\n","\n","### ‚úÖ Example: Extract Named Entities\n","\n","```python\n","from nltk import ne_chunk\n","from nltk.tree import Tree\n","\n","def extract_named_entities(text):\n","    tokens = word_tokenize(text)\n","    tagged = pos_tag(tokens)\n","    chunked = ne_chunk(tagged)\n","\n","    named_entities = []\n","    for chunk in chunked:\n","        if isinstance(chunk, Tree):\n","            named_entity = \" \".join(c[0] for c in chunk)\n","            named_entities.append(named_entity)\n","    return named_entities\n","\n","text = \"Shakil studies at Daffodil International University in Dhaka.\"\n","print(extract_named_entities(text))  # ['Shakil', 'Daffodil International University', 'Dhaka']\n","```\n","\n","You can also **remove** them by filtering these out from your tokens.\n","\n","---\n","\n","## üîπ 3. **TF-IDF Vectorization**\n","\n","TF-IDF (Term Frequency-Inverse Document Frequency) is a technique to convert text into numerical form **based on importance**, not just frequency.\n","\n","### ‚úÖ Example with `TfidfVectorizer`\n","\n","```python\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","corpus = [\n","    \"I love programming in Python\",\n","    \"Python is great for natural language processing\",\n","    \"Natural language processing is fun\"\n","]\n","\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Print TF-IDF feature names and values\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())\n","```\n","\n","---\n","\n","### üß† Summary Table\n","\n","| Feature        | Library                   | Use Case                                |\n","| -------------- | ------------------------- | --------------------------------------- |\n","| POS Tagging    | `nltk.pos_tag()`          | Keep only nouns/verbs/etc.              |\n","| Named Entities | `nltk.ne_chunk()`         | Extract or remove proper names          |\n","| TF-IDF Vector  | `sklearn.TfidfVectorizer` | Convert clean text into numeric vectors |\n","\n","---\n","\n","Would you like me to **combine these in one full pipeline**, from text to vector form, ready for machine learning?\n"],"metadata":{"id":"sdz0IffCook5"}},{"cell_type":"code","source":["import nltk\n","from nltk import pos_tag, word_tokenize, ne_chunk, CFG, ChartParser\n","from nltk.tree import Tree\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Download required NLTK models\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","# -------- 1. POS Tagging: Keep Only Nouns and Verbs -------- #\n","def filter_by_pos(text):\n","    tokens = word_tokenize(text)\n","    tagged = pos_tag(tokens)\n","    filtered = [word for word, tag in tagged if tag.startswith(\"NN\") or tag.startswith(\"VB\")]\n","    return filtered\n","\n","# -------- 2. Named Entity Recognition (NER) -------- #\n","def extract_named_entities(text):\n","    tokens = word_tokenize(text)\n","    tagged = pos_tag(tokens)\n","    chunked = ne_chunk(tagged)\n","    named_entities = []\n","    for chunk in chunked:\n","        if isinstance(chunk, Tree):\n","            named_entity = \" \".join(c[0] for c in chunk)\n","            named_entities.append(named_entity)\n","    return named_entities\n","\n","# -------- 3. Constituency Parsing (Simple Grammar Example) -------- #\n","def parse_sentence(text):\n","    # Define a mini grammar that matches your example\n","    grammar = CFG.fromstring(\"\"\"\n","        S -> NP VP\n","        NP -> Det N | N\n","        VP -> V NP | V\n","        Det -> 'the' | 'a'\n","        N -> 'Shakil' | 'language' | 'processing' | 'university'\n","        V -> 'studies' | 'is'\n","    \"\"\")\n","    parser = ChartParser(grammar)\n","\n","    tokens = word_tokenize(text)\n","    for tree in parser.parse(tokens):\n","        tree.pretty_print()\n","        return tree  # Return first tree\n","\n","# -------- 4. TF-IDF Vectorization -------- #\n","def tfidf_vectorize(corpus):\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(corpus)\n","    print(\"Features:\", vectorizer.get_feature_names_out())\n","    print(\"TF-IDF Matrix:\\n\", X.toarray())\n","    return X\n","\n","# ---------- Example Usage ---------- #\n","text = \"Shakil studies at Daffodil International University in Dhaka.\"\n","\n","print(\"\\n1Ô∏è‚É£ POS Filtered Tokens:\")\n","print(filter_by_pos(text))\n","\n","print(\"\\n2Ô∏è‚É£ Named Entities:\")\n","print(extract_named_entities(text))\n","\n","print(\"\\n3Ô∏è‚É£ Parse Tree:\")\n","parse_sentence(\"Shakil studies language\")  # Keep this simple to match the grammar\n","\n","print(\"\\n4Ô∏è‚É£ TF-IDF Vectorization:\")\n","corpus = [\n","    \"Shakil studies language\",\n","    \"Natural language processing is amazing\",\n","    \"Shakil loves Python\"\n","]\n","tfidf_vectorize(corpus)\n"],"metadata":{"id":"fzIsWAzhmN9a"},"execution_count":null,"outputs":[]}]}
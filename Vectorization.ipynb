{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOZtuh27NvxsctRki1ENNX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zy-BbDAHeB97"},"outputs":[],"source":["# ‚úÖ TfidfVectorizer() ‚Äì Short Explanation\n","# TfidfVectorizer is a tool from Scikit-learn used to convert a collection of text documents into numerical feature vectors using TF-IDF weighting.\n","\n","\n","import re\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","\n","# 1. Sample corpus + labels (for a simple sentiment task)\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# 2. Text Preprocessing\n","stemmer = PorterStemmer()\n","stops = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    # lowercase\n","    text = text.lower()\n","    # remove punctuation\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","    # tokenize + remove stopwords + stem\n","    tokens = [\n","        stemmer.stem(tok)\n","        for tok in text.split()\n","        if tok not in stops\n","    ]\n","    return \" \".join(tokens)\n","\n","docs_clean = [preprocess(d) for d in docs]\n","\n","# 3. Syntax & Semantics (basic POS tagging + regex-based NER)\n","import nltk\n","nltk.download('averaged_perceptron_tagger', quiet=True)\n","\n","def syntax_semantics(text):\n","    tokens = text.split()\n","    pos_tags = nltk.pos_tag(tokens)\n","    # super-simple ‚ÄúNER‚Äù: any capitalized word in original text\n","    ner = re.findall(r\"\\b[A-Z][a-z]+\\b\", text)\n","    return pos_tags, ner\n","\n","for orig in docs:\n","    pos, ner = syntax_semantics(orig)\n","    print(f\"\\nORIGINAL: {orig}\")\n","    print(\" POS TAGS:\", pos)\n","    print(\" NER:\", ner)\n","\n","# 4. Vectorization + 5. Modeling (TF-IDF + Logistic Regression)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    docs_clean, labels, test_size=0.5, random_state=42\n",")\n","\n","pipeline = Pipeline([\n","    (\"tfidf\", TfidfVectorizer()),\n","    (\"clf\", LogisticRegression())\n","])\n","\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate\n","acc = pipeline.score(X_test, y_test)\n","print(f\"\\nTest accuracy: {acc:.2f}\")\n","\n","# Predict on new sentence\n","new = \"That movie was absolutely wonderful and thrilling.\"\n","new_clean = preprocess(new)\n","pred = pipeline.predict([new_clean])[0]\n","print(f\"New sentence prediction (1=pos,0=neg): {pred}\")\n"]},{"cell_type":"markdown","source":["Certainly! Here's how you can integrate **Word2Vec** and **BERT** with a pipeline, similar to how you used **TF-IDF** with `LogisticRegression`.\n","\n","### 1. **Word2Vec Integration with Pipeline**\n","\n","We'll create a custom function for Word2Vec, and then integrate it into a Scikit-learn pipeline.\n","\n","```python\n","from gensim.models import Word2Vec\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","\n","# Sample corpus + labels\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# Preprocessing\n","stemmer = PorterStemmer()\n","stop = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","    tokens = [stemmer.stem(tok) for tok in text.split() if tok not in stop]\n","    return tokens\n","\n","docs_clean = [preprocess(d) for d in docs]\n","\n","# Word2Vec training\n","model = Word2Vec(sentences=docs_clean, vector_size=50, window=5, min_count=1)\n","\n","# Custom Transformer for Word2Vec embedding conversion\n","class Word2VecTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, model):\n","        self.model = model\n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X):\n","        return np.array([self.document_to_vector(doc) for doc in X])\n","    \n","    def document_to_vector(self, doc):\n","        vectors = [self.model.wv[word] for word in doc if word in self.model.wv]\n","        if vectors:\n","            return np.mean(vectors, axis=0)\n","        else:\n","            return np.zeros(self.model.vector_size)\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(docs_clean, labels, test_size=0.5, random_state=42)\n","\n","# Create pipeline with Word2Vec and Logistic Regression\n","pipeline = Pipeline([\n","    (\"word2vec\", Word2VecTransformer(model)),\n","    (\"clf\", LogisticRegression())\n","])\n","\n","# Train the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate\n","acc = pipeline.score(X_test, y_test)\n","print(f\"Test accuracy with Word2Vec: {acc:.2f}\")\n","\n","# Predict on new sentence\n","new = \"That movie was absolutely wonderful and thrilling.\"\n","new_clean = preprocess(new)\n","pred = pipeline.predict([new_clean])[0]\n","print(f\"New sentence prediction (1=pos, 0=neg): {pred}\")\n","```\n","\n","### 2. **BERT Integration with Pipeline**\n","\n","Now, let's integrate **BERT** using the Hugging Face `transformers` library into a pipeline. BERT requires tokenization and encoding of text before we can use it for classification. We can create a custom transformer to handle this as well.\n","\n","```python\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","import torch\n","import numpy as np\n","\n","# Sample corpus + labels\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# Preprocessing function for BERT (lowercasing)\n","def preprocess(text):\n","    return text.lower()\n","\n","# Tokenization and BERT encoding\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","\n","class BertTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, model, tokenizer):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","    \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def transform(self, X):\n","        return np.array([self.encode_with_bert(doc) for doc in X])\n","    \n","    def encode_with_bert(self, text):\n","        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n","        with torch.no_grad():\n","            outputs = self.model(**inputs)\n","        return outputs.logits.squeeze().numpy()\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(docs, labels, test_size=0.5, random_state=42)\n","\n","# Create pipeline with BERT and Logistic Regression\n","pipeline = Pipeline([\n","    (\"bert\", BertTransformer(model, tokenizer)),\n","    (\"clf\", LogisticRegression())\n","])\n","\n","# Train the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate\n","acc = pipeline.score(X_test, y_test)\n","print(f\"Test accuracy with BERT: {acc:.2f}\")\n","\n","# Predict on new sentence\n","new = \"That movie was absolutely wonderful and thrilling.\"\n","new_clean = preprocess(new)\n","pred = pipeline.predict([new_clean])[0]\n","print(f\"New sentence prediction (1=pos, 0=neg): {pred}\")\n","```\n","\n","### **Explanation**:\n","\n","* **Word2Vec**: We're using a custom transformer to convert the documents into vectors by averaging word vectors from Word2Vec.\n","* **BERT**: Similarly, we created a custom transformer that uses BERT to encode each document and then classify it with Logistic Regression.\n","\n","---\n","\n","### üß© What This Achieves:\n","\n","* **Word2Vec**: Captures semantic word relationships, ideal for simpler NLP tasks.\n","* **BERT**: Leverages state-of-the-art, context-aware embeddings for more accurate predictions.\n","\n","Would you like to test the pipeline with different types of text or need any further adjustments?\n"],"metadata":{"id":"FZqUnVlmfYaX"}},{"cell_type":"markdown","source":["Sure! Let's integrate **Word2Vec** and **Transformers** into your pipeline for advanced vectorization. I'll show you how to replace **TF-IDF** with these methods and update the rest of the pipeline accordingly.\n","\n","### 1. **Word2Vec Integration**\n","\n","Word2Vec provides dense vector representations of words, capturing semantic meanings. You can replace TF-IDF with Word2Vec, but we'll need to use **Gensim** for Word2Vec.\n","\n","### **Steps**:\n","\n","* Use **Word2Vec** to convert each word into a vector.\n","* Average the word vectors of each document to represent the entire document.\n","\n","#### Word2Vec Example with your Sentiment Analysis Pipeline\n","\n","First, let's modify your pipeline to use Word2Vec.\n","\n","```python\n","import gensim\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from nltk.corpus import stopwords\n","import re\n","from nltk.stem import PorterStemmer\n","\n","# Sample corpus and labels (sentiment task)\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# Preprocessing\n","stemmer = PorterStemmer()\n","stop = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    # Lowercase and remove punctuation\n","    text = text.lower()\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","    tokens = [stemmer.stem(tok) for tok in text.split() if tok not in stop]\n","    return tokens\n","\n","docs_clean = [preprocess(d) for d in docs]\n","\n","# Word2Vec training\n","# Tokenize all documents\n","all_tokens = [token for doc in docs_clean for token in doc]\n","\n","# Create Word2Vec model\n","model = gensim.models.Word2Vec(sentences=docs_clean, vector_size=50, window=5, min_count=1)\n","\n","# Function to convert document to vector (average word vectors)\n","def document_to_vector(doc):\n","    vectors = [model.wv[word] for word in doc if word in model.wv]\n","    if vectors:\n","        return np.mean(vectors, axis=0)\n","    else:\n","        return np.zeros(model.vector_size)\n","\n","# Convert documents to vectors\n","X = np.array([document_to_vector(doc) for doc in docs_clean])\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n","\n","# Train the Logistic Regression model\n","clf = LogisticRegression()\n","clf.fit(X_train, y_train)\n","\n","# Evaluate\n","acc = clf.score(X_test, y_test)\n","print(f\"Test accuracy with Word2Vec: {acc:.2f}\")\n","\n","# Predict on new sentence\n","new = \"That movie was absolutely wonderful and thrilling.\"\n","new_clean = preprocess(new)\n","new_vector = document_to_vector(new_clean)\n","pred = clf.predict([new_vector])[0]\n","print(f\"New sentence prediction (1=pos,0=neg): {pred}\")\n","```\n","\n","---\n","\n","### 2. **Transformer Integration (BERT)**\n","\n","BERT (Bidirectional Encoder Representations from Transformers) is more powerful and context-aware. We can use the **Hugging Face `transformers` library** to integrate a pre-trained BERT model for encoding.\n","\n","#### Steps for BERT:\n","\n","* Load a pre-trained BERT model from Hugging Face.\n","* Tokenize input text and convert it into BERT‚Äôs input format.\n","* Use BERT to generate embeddings for the input text.\n","\n","### **Example using BERT for Sentiment Classification**:\n","\n","```python\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.pipeline import Pipeline\n","import torch\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","# Sample corpus + labels\n","docs = [\n","    \"I loved the new Batman movie!\",\n","    \"The plot was boring and too long.\",\n","    \"What a fantastic performance by the lead actors.\",\n","    \"I hated the ending, it was terrible.\"\n","]\n","labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","# Preprocessing\n","def preprocess(text):\n","    return text.lower()\n","\n","# Use BERT Tokenizer to convert text into input format for BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","\n","# Tokenization and encoding function for BERT input\n","def encode_with_bert(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    return outputs.logits.squeeze().numpy()\n","\n","# Convert documents to BERT embeddings\n","X = np.array([encode_with_bert(doc) for doc in docs])\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n","\n","# Train the Logistic Regression model\n","clf = LogisticRegression()\n","clf.fit(X_train, y_train)\n","\n","# Evaluate\n","acc = clf.score(X_test, y_test)\n","print(f\"Test accuracy with BERT: {acc:.2f}\")\n","\n","# Predict on new sentence\n","new = \"That movie was absolutely wonderful and thrilling.\"\n","new_clean = preprocess(new)\n","new_vector = encode_with_bert(new_clean)\n","pred = clf.predict([new_vector])[0]\n","print(f\"New sentence prediction (1=pos,0=neg): {pred}\")\n","```\n","\n","---\n","\n","### üß© Comparison Summary:\n","\n","| Vectorization Type | Strengths                                | Use Case                                            |\n","| ------------------ | ---------------------------------------- | --------------------------------------------------- |\n","| **Word2Vec**       | Captures semantic meaning of words.      | Simple NLP tasks, small datasets.                   |\n","| **BERT**           | Context-aware embeddings. Very powerful. | Complex NLP tasks, large datasets, better accuracy. |\n","\n","---\n","\n","Would you like help setting up a specific part of the pipeline or need more details on any of these vectorizers?\n"],"metadata":{"id":"Ma0ISTHHfUR4"}},{"cell_type":"code","source":[],"metadata":{"id":"A1i9ahR5fVS_"},"execution_count":null,"outputs":[]}]}